{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Download\n",
    "The [Hindi Text Short Summarization Corpus](https://www.kaggle.com/disisbig/hindi-text-short-summarization-corpus#) is used for this experiment.\n",
    "This cell will download and extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p dataset\n",
    "!wget \"https://storage.googleapis.com/kaggle-data-sets/465845/875288/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20200920%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20200920T140724Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=65a3f8130fe23114ad7957d0be8b3df7237254eb40655fb6d203eeff28fd7e54a41e84a54c17b19d71404c5192af3c2f9ab8eea429e203ed8d04f4d2f3a9ceadc80082705c10fef97034a4e5cb1d1e3325f58f7de2c40e2d7a60443e467098c306e0cbafd91dc60f1e7636c7cbcd901f46f6f860893bb7b7cb6728acc62ed323ab5c1346f23216c932a0c04d34cde001eb7b8030e21f76bd7c246c94114f3f6897ef28f55a9d998e17f56e4148a833c8f3fe1128709b07f9cc9902c9e686301ad6db222a86e30b0a5a9522fc005842361f4288736571a095f038b2d57ed2ecfcb9077d368f52289bdb19eff1b7261b5d76b487108ab2deee5df843d659b88cf9\" -O dataset/dataset-archive.zip\n",
    "!unzip dataset-archive.zip -d dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We have a huge collection of news articles and their corresponding headlines. Since this is a word-meaning modelling task, we don't need the correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports!\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the raw data\n",
    "df1 = pd.read_csv('dataset/train.csv', delimiter=',')\n",
    "df1.dataframeName = 'train.csv'\n",
    "nRow, nCol = df1.shape\n",
    "train_articles = df1['article']\n",
    "train_headlines = df1['headline']\n",
    "print(f'There are {nRow} rows and {nCol} columns')\n",
    "print(df1.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the string, remove URLs, emoji, numeral-numbers, multiple spaces and non-string entities. \n",
    "def prep(str):\n",
    "    try:\n",
    "        regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        str = re.sub(r'[a-zA-Z]+', '', str)\n",
    "        str = re.sub(emoji_pattern, '', str)\n",
    "        str = re.sub(regex, \"\", str)\n",
    "        str = str.translate(str.maketrans('', '', string.punctuation))\n",
    "        str = re.sub(r\"\\d+\", \"\", str)\n",
    "        str = str.replace(\"\\n\", \"\")\n",
    "        str = re.sub(' +', ' ', str)\n",
    "        return str.strip()\n",
    "    except TypeError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store intermedieate data.\n",
    "!mkdir -p intermedieates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_articles = []\n",
    "final_headlines = []\n",
    "for art, head in zip(train_articles, train_headlines):\n",
    "    if art is not None and head is not None:\n",
    "        prep_head = prep(head)\n",
    "        prep_art = prep(art)\n",
    "        if prep_head is not None and prep_art is not None:\n",
    "            len_art = len(prep_art.split())\n",
    "            len_head = len(prep_head.split())\n",
    "            if len_art < 800 and len_head < 15:\n",
    "                final_articles.append(prep_art)\n",
    "                final_headlines.append(prep_head)\n",
    "print(f'selected {len(final_articles)} articles out of {nRow}, {100*len(final_articles)/nRow}% selected')\n",
    "with open(\"intermedieates/headlines.txt\", \"w\", encoding=\"utf-8\") as headlines_file:\n",
    "    with open(\"intermedieates/articles.txt\", \"w\", encoding=\"utf-8\") as articles_file:\n",
    "        for head, article in zip(final_headlines, final_articles):\n",
    "            headlines_file.write(head)\n",
    "            headlines_file.write(\"\\n\")\n",
    "            articles_file.write(article)\n",
    "            articles_file.write(\"\\n\")\n",
    "print(\"Written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Words\n",
    "It is not possible to account for every single word in the vocublary, so we choose the 16384 most frequent ones to make vectors out of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermedieates/articles.txt\", \"r\") as articles_file:\n",
    "    articles = articles_file.readlines()\n",
    "with open(\"intermedieates/headlines.txt\", \"r\") as headlines_file:\n",
    "    headlines = headlines_file.readlines()\n",
    "words_and_freq = {}\n",
    "for article in articles:\n",
    "    for word in article.split():\n",
    "        try:\n",
    "            words_and_freq[word] += 1\n",
    "        except KeyError:\n",
    "            words_and_freq[word] = 1\n",
    "\n",
    "for headline in headlines:\n",
    "    for word in headline.split():\n",
    "        try:\n",
    "            words_and_freq[word] += 1\n",
    "        except KeyError:\n",
    "            words_and_freq[word] = 1\n",
    "\n",
    "pickle.dump(words_and_freq, open('intermedieates/words_and_freq.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intermedieates/articles.txt\", \"r\") as articles_file:\n",
    "    articles = articles_file.readlines()\n",
    "with open(\"intermedieates/headlines.txt\", \"r\") as headlines_file:\n",
    "    headlines = headlines_file.readlines()\n",
    "words_and_freq = pickle.load(open(\"intermedieates/words_and_freq.pkl\", 'rb'))\n",
    "words_new = [\"<m>\", \"<s>\", \"<e>\", \"<d>\", \"<u>\"]\n",
    "for w in sorted(words_and_freq, key=words_and_freq.get, reverse=True):\n",
    "    words_new.append(w)\n",
    "    if len(words_new) >= 16384:\n",
    "        break\n",
    "\n",
    "final_articles = []\n",
    "final_headlines = []\n",
    "for article, headline in tqdm(zip(articles, headlines)):\n",
    "    article_set = set(article.split())\n",
    "    headline_set = set(headline.split())\n",
    "    if len(article_set) > 0 and len(headline_set) > 0:\n",
    "        article_diff = article_set.difference(words_new)\n",
    "        headline_diff = headline_set.difference(words_new)\n",
    "        unk_ratio_article = len(article_diff) / len(article_set)\n",
    "        unk_num_headline = len(headline_diff) / len(headline_set)\n",
    "        if unk_ratio_article < 0.1 and unk_num_headline < 0.1:\n",
    "            final_articles.append(article)\n",
    "            final_headlines.append(headline)\n",
    "vocab_head_art = {\n",
    "    \"vocabulary\": words_new,\n",
    "    \"articles\": final_articles,\n",
    "    \"headlines\": final_headlines\n",
    "}\n",
    "pickle.dump(vocab_head_art, open(\"intermedieates/vocab_head_art.pkl\", \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_head_art = pickle.load(open(\"intermedieates/vocab_head_art.pkl\", \"rb\"))\n",
    "vocab = vocab_head_art[\"vocabulary\"]\n",
    "articles = vocab_head_art[\"articles\"]\n",
    "headlines = vocab_head_art[\"headlines\"]\n",
    "\n",
    "word2id = {}\n",
    "id2word = {}\n",
    "for id, word in enumerate(vocab):\n",
    "    word2id[word] = id\n",
    "    id2word[id] = word\n",
    "\n",
    "unk_id = word2id[\"<u>\"]\n",
    "start_id = word2id[\"<s>\"]\n",
    "end_id = word2id[\"<e>\"]\n",
    "id2freq = [0] * len(vocab)\n",
    "\n",
    "\n",
    "def get_id(word):\n",
    "    try:\n",
    "        id = word2id[word]\n",
    "    except KeyError:\n",
    "        id = unk_id\n",
    "    id2freq[id] += 1\n",
    "    return id\n",
    "\n",
    "\n",
    "articles_tokenized = []\n",
    "headlines_tokenized = []\n",
    "for article in articles:\n",
    "    id2freq[start_id] += 1\n",
    "    id2freq[end_id] += 1\n",
    "    articles_tokenized.append([start_id] + [get_id(x) for x in article.split()] + [end_id])\n",
    "\n",
    "for headline in headlines:\n",
    "    id2freq[start_id] += 1\n",
    "    id2freq[end_id] += 1\n",
    "    headlines_tokenized.append([start_id] + [get_id(x) for x in headline.split()] + [end_id])\n",
    "\n",
    "final_data = {\n",
    "    \"word2id\": word2id,\n",
    "    \"id2word\": id2word,\n",
    "    \"id2freq\": id2freq,\n",
    "    \"headlines_tokenized\": headlines_tokenized,\n",
    "    \"articles_tokenized\": articles_tokenized\n",
    "}\n",
    "pickle.dump(final_data, open(\"intermedieates/final_data.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p saved-tf-models outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class supplies pairs of words to our algorithm on demand\n",
    "class WordPairSupplier:\n",
    "    def __init__(self, CONTEXT_SIZE=2):\n",
    "        data = pickle.load(open(\"intermedieates/final_data.pkl\", \"rb\"))\n",
    "        self.articles = data[\"articles_tokenized\"]\n",
    "        self.headlines = data[\"headlines_tokenized\"]\n",
    "        self.vals = list(range(-CONTEXT_SIZE, 0)) + list(range(1, CONTEXT_SIZE + 1))\n",
    "        self.id2freq = np.asarray(data['id2freq'])\n",
    "        self.neg_probs = np.power(self.id2freq, 3 / 4)\n",
    "        self.neg_probs /= np.sum(self.neg_probs)\n",
    "        self.ids = np.arange(self.neg_probs.shape[0])\n",
    "        self.word2id = data['word2id']\n",
    "        self.id2word = data['id2word']\n",
    "        self.freqs_three_fourths = [0] * 16384\n",
    "\n",
    "    def get_skip_prob(self, id):\n",
    "        return 1 - sqrt((1e3 / self.id2freq[id]))\n",
    "\n",
    "    def get_pair(self):\n",
    "        str = random.choice(self.articles)\n",
    "        L = len(str) - 1\n",
    "        base_index = random.randint(0, L)\n",
    "        base_id = str[base_index]\n",
    "        if random.random() < self.get_skip_prob(base_id):\n",
    "            return self.get_pair()\n",
    "        offset = random.choice(self.vals)\n",
    "        target_index = base_index + offset\n",
    "        target_index = max(0, min(target_index, L))\n",
    "        target_id = str[target_index]\n",
    "        if random.random() < self.get_skip_prob(target_id):\n",
    "            return self.get_pair()\n",
    "        return base_id, target_id\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        xs = []\n",
    "        ys = []\n",
    "        for _ in range(batch_size):\n",
    "            x, y = self.get_pair()\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "        xs = np.asarray(xs)\n",
    "        ys = np.asarray(ys)\n",
    "        xs = to_categorical(xs, 16384)\n",
    "        ys = to_categorical(ys, 16384)\n",
    "        return xs, ys\n",
    "\n",
    "    def get_batch_raw(self, batch_size, k):\n",
    "        xs = []\n",
    "        ys = []\n",
    "        negs = []\n",
    "        for _ in range(batch_size):\n",
    "            x, y = self.get_pair()\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "            negs.append(self.get_negative_samples(k))\n",
    "        xs = np.asarray(xs)\n",
    "        ys = np.asarray(ys)\n",
    "        negs = np.asarray(negs)\n",
    "        return xs, ys, negs\n",
    "\n",
    "    def get_negative_samples(self, k):\n",
    "        return np.random.choice(self.ids, k, p=self.neg_probs)\n",
    "\n",
    "    def get_word(self, id):\n",
    "        return self.id2word[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The actual algorithm\n",
    "An implementation of the method from [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tensorflow Implementation\n",
    "The naive skip-gram model, without any optimizations is implemented in the cell below, it is easily representable using the building blocks that TensorFlow provides.\n",
    "The NumPy implementation that includes negative sampling comes ahead.\n",
    "Stop running the below cell whenever you want, checkpoints are saved regularly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(layers=[\n",
    "    Dense(256, activation=None, use_bias=False),\n",
    "    Dense(16384, activation='softmax', use_bias=False)\n",
    "])\n",
    "model.compile(optimizer=Adam(0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, 16384))\n",
    "model.summary()\n",
    "supplier = WordPairSupplier()\n",
    "\n",
    "\n",
    "def data_generator(num_ex):\n",
    "    def generate_batch():\n",
    "        for _ in range(num_ex):\n",
    "            yield supplier.get_batch(512)\n",
    "\n",
    "    return generate_batch\n",
    "\n",
    "\n",
    "saver = ModelCheckpoint(\"saved-tf-models/saved_embeddings_model.h5\", monitor=\"loss\", save_best_only=True)\n",
    "dataset = tf.data.Dataset.from_generator(data_generator(2048), (tf.float32, tf.float32),\n",
    "                                         (tf.TensorShape([None, 16384]), tf.TensorShape([None, 16384])))\n",
    "model.fit(dataset, epochs=16384, callbacks=[saver])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next cell will extract the weights from the saved checkpoint of the model that was trained in the previous cell.\n",
    "After running the cell you should see labels.tsv and vectors.tsv in the outputs folder, go to the [Embedding Projector](https://projector.tensorflow.org/) to visualize these."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"saved-tf-models/saved_embeddings_model.h5\")\n",
    "weights = model.weights\n",
    "embeddings = weights[0].numpy()\n",
    "words = ['नहीं', 'किया', 'समय', 'पहचान']\n",
    "supplier = WordPairSupplier()\n",
    "for word in words:\n",
    "    word_id = supplier.word2id[word]\n",
    "    word_vector = embeddings[word_id]\n",
    "    cosine_dists = cosine_similarity(np.asarray([word_vector]), embeddings)[0]\n",
    "    indxs = np.argsort(cosine_dists)[::-1]\n",
    "    for j in range(5):\n",
    "        print(supplier.id2word[indxs[j]])\n",
    "    print()\n",
    "with open(\"outputs/vectors.tsv\", 'w') as vectors_out:\n",
    "    with open(\"outputs/labels.tsv\", 'w') as labels_out:\n",
    "        for id, vector in enumerate(embeddings):\n",
    "            for val in vector:\n",
    "                vectors_out.write(str(val) + \"\\t\")\n",
    "            vectors_out.write(\"\\n\")\n",
    "            labels_out.write(supplier.id2word[id] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### NumPy Implementation\n",
    "The NumPy implementation with negative sampling to speed up the process is implemented in the next few cells\n",
    "My notes which might offer an explanation are in the repo too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.auto import trange\n",
    "\n",
    "VOCAB_SIZE = 16384\n",
    "DIM = 256\n",
    "word_vectors = np.random.randn(VOCAB_SIZE * DIM).reshape((VOCAB_SIZE, DIM)) / 10000\n",
    "context_vectors = np.random.randn(VOCAB_SIZE * DIM).reshape((VOCAB_SIZE, DIM)) / 10000\n",
    "LR = 0.01\n",
    "\n",
    "\n",
    "def one_pass_increment(center, target):\n",
    "    coeff = (1 / (np.exp(word_vectors[center] * context_vectors[target]) + 1))\n",
    "    word_vectors[center] += (context_vectors[target] * coeff) * LR\n",
    "    context_vectors[target] += (word_vectors[center] * coeff) * LR\n",
    "\n",
    "\n",
    "def one_pass_neg(centers, negs):\n",
    "    for center, neg in zip(centers, negs):\n",
    "        center = np.repeat(center, negs.shape[1])\n",
    "        coeff = (1 / (np.exp(word_vectors[center] * context_vectors[neg]) + 1))\n",
    "        word_vectors[center] -= (context_vectors[neg] * coeff) * LR\n",
    "        context_vectors[neg] -= (word_vectors[center] * coeff) * LR\n",
    "\n",
    "\n",
    "supplier = WordPairSupplier()\n",
    "for i in trange(1000):\n",
    "    batch = supplier.get_batch_raw(4096, 8)\n",
    "    one_pass_increment(batch[0], batch[1])\n",
    "    one_pass_neg(batch[0], batch[2])\n",
    "    if i % 50 == 0:\n",
    "        print(\"Saving for \", i)\n",
    "        np.save(\"word_vectors.npy\", word_vectors)\n",
    "        np.save(\"context_vectors.npy\", context_vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}